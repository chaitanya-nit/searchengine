{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install required libraries. You need to restart runtime after installation.\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab,elasticsearch,inference]\n",
    "pip install datasets\n",
    "pip install apache-beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Colab / No Docker environments: Start Elasticsearch from source\n",
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "! chown -R daemon:daemon elasticsearch-7.9.2\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "es_server = Popen(\n",
    "    [\"elasticsearch-7.9.2/bin/elasticsearch\"], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    ")\n",
    "# wait until ES has started\n",
    "! sleep 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document store\n",
    "\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the preprocessed wikipedia simple english from huggingface datasets library\n",
    "from datasets import load_dataset\n",
    "simple_ds = load_dataset(\"wikipedia\", \"20220301.simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the content from dataset object and transform it to Document class\n",
    "from tqdm import tqdm\n",
    "from haystack.schema import Document\n",
    "\n",
    "raw_document_list = []\n",
    "for file in tqdm(simple_ds['train']):\n",
    "    temp_document = {}\n",
    "    temp_document[\"content_type\"] = \"text\"\n",
    "    temp_document[\"content\"] = file[\"text\"]\n",
    "    temp_document[\"meta\"] = {\"id\":file[\"id\"],\"url\":file[\"url\"],\"title\":file[\"title\"]}\n",
    "    temp_document[\"id_hash_keys\"] = [\"content\",\"meta\"]\n",
    "    raw_document_list.append(Document.from_dict(temp_document))\n",
    "\n",
    "print(f\"Number of articles present in the wikipedia simple english dataset are {len(raw_document_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the demo, we will use 10k articles from the dataset\n",
    "\n",
    "from haystack.nodes import PreProcessor\n",
    "preprocessor = PreProcessor(split_by=\"word\",split_respect_sentence_boundary=True,split_length=100)\n",
    "processed_document_list = preprocessor.process(documents=raw_document_list[:10000])\n",
    "\n",
    "# Write the processed documents to document_store\n",
    "\n",
    "document_store.write_documents(documents=processed_document_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using sentence transformer model\n",
    "\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "embedding_retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    model_format=\"sentence_transformers\",top_k=100)\n",
    "\n",
    "# Generate embeddings\n",
    "document_store.update_embeddings(retriever=embedding_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the document count and embeddings count\n",
    "print(f\"Document count after writing to index are {document_store.get_document_count()}\")\n",
    "print(f\"Embedding count present in current index are {document_store.get_embedding_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Keyword search and Join node\n",
    "from haystack.nodes import BM25Retriever,JoinDocuments\n",
    "keyword_retriever = BM25Retriever(document_store=document_store,top_k=100)\n",
    "join_docs = JoinDocuments(join_mode=\"concatenate\",weights=[0.5,0.5],top_k_join=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets glue the components using a pipeline\n",
    "\n",
    "from haystack.pipelines import Pipeline\n",
    "hybrid_search_pipeline = Pipeline()\n",
    "\n",
    "#Adding Semantic Component\n",
    "hybrid_search_pipeline.add_node(component=embedding_retriever,name=\"semantic-search\",inputs=[\"Query\"])\n",
    "\n",
    "# Adding Keyword Component\n",
    "hybrid_search_pipeline.add_node(component=keyword_retriever,name=\"keyword-search\",inputs=[\"Query\"])\n",
    "\n",
    "# Join Documents\n",
    "hybrid_search_pipeline.add_node(component=join_docs,name=\"join-documents\",inputs=[\"semantic-search,keyword-search\"])\n",
    "results = hybrid_search_pipeline.run(query=\"What is the Islamic Republic Day?\",params={\"top_k\":100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print the results\n",
    "from haystack.utils import print_documents\n",
    "print_documents(results=results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack-recent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
